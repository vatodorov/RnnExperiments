{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a trained LSTM for time series forecasts.\n",
    "Author: Valentin Todorov\n",
    "\"\"\"\n",
    "\n",
    "# Import the packages I'll need\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "### Provide input values to parameters\n",
    "# Read in the data\n",
    "cwd = os.getcwd()\n",
    "dataLocation = \"/Users/valentin/Google Drive/Data/\"\n",
    "inputFile = \"file_for_lstm_model_logdiff\"\n",
    "\n",
    "inputDf = pd.read_csv(dataLocation + inputFile + \".csv\")\n",
    "inputDf.head(10)\n",
    "\n",
    "# Remove the forecasts from the data\n",
    "# inputData.Price[[inputData.Date > 2017-01-01 00:00:00]] = 0\n",
    "\n",
    "# Format \"Date\" field as date\n",
    "inputDf[['date']] = pd.to_datetime(inputDf.date)\n",
    "\n",
    "# Convert the data frame to a Numpy array\n",
    "targetArr = inputDf.iloc[:, 0:].values\n",
    "\n",
    "# Select random seed\n",
    "randSeed = 7896\n",
    "\n",
    "# Provide names of input features\n",
    "inputDataFrame = inputDf\n",
    "inputData = targetArr\n",
    "\n",
    "# Create a Numpy array from the input data\n",
    "dataframe = inputData[:, 1:, ]\n",
    "# xVarColumns = [1, 2, 3]                                   # Select features: BY DEFAULT, it uses all features in columns 1:END for predictors\n",
    "yVarColumns = [0]                                           # Select target: The target should always be in the first column\n",
    "number_of_features = len(list(inputDf)) - 2            # Calculate the number of features to be used in the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Sanity checks of the data\n",
    "########################################################\n",
    "\n",
    "print(number_of_features, \"\\nThe analytical array shape is (includes the target):\", dataframe.shape)\n",
    "inputDf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Transform the data\n",
    "########################################################\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(randSeed)\n",
    "\n",
    "## NO NEED FOR ARRAYS - Extract the NumPy array from the dataframe and convert the integer values to\n",
    "# floating point values, which are more suitable for modeling with a neural network\n",
    "# dataset = dataframe.values\n",
    "# dataset = dataset.astype('float32')\n",
    "\n",
    "# Normalize the data between 0 - 1\n",
    "scaler = MinMaxScaler(feature_range = (0, 1), copy = True)\n",
    "dataset = scaler.fit_transform(dataframe)\n",
    "\n",
    "# Split the data in train and validaiton\n",
    "trainSize = 166                         # This is the number of records used in the training set (03/2000 - 12/2013)\n",
    "train = dataset[0:trainSize, :]\n",
    "validate = dataset[trainSize:len(dataset), :]\n",
    "\n",
    "## Modify the data for the LSTM network - The LSTM network expects the input data (X)\n",
    "# to be provided with a specific array structure in the form of: [samples, time steps, features].\n",
    "trainX = train[:, 1:]\n",
    "validateX = validate[:, 1:]\n",
    "\n",
    "trainY = train[:, yVarColumns]\n",
    "validateY = validate[:, yVarColumns]\n",
    "\n",
    "dataframe_length = len(trainY)\n",
    "# dataframe_dim = Need to figure out how to count the columns of the array\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = trainX.reshape(trainX.shape[0], 1, trainX.shape[1])\n",
    "validateX = validateX.reshape(validateX.shape[0], 1, validateX.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Sanity checks of the data\n",
    "########################################################\n",
    "\n",
    "print(len(train), len(validate), len(dataset))\n",
    "print(trainX.shape)\n",
    "\n",
    "train[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Create model\n",
    "########################################################\n",
    "\n",
    "## The LSTM network expects the input data (X) to be provided with a specific\n",
    "# array structure in the form of: [samples, time steps, features]\n",
    "# Define the network\n",
    "\n",
    "modelFit = Sequential()\n",
    "modelFit.add(LSTM(10,\n",
    "                  #activation = 'sigmoid',\n",
    "                  input_shape = (1, number_of_features)))\n",
    "modelFit.add(Dropout(.05))\n",
    "#modelFit.add(Dense(10, activation = 'relu'))\n",
    "modelFit.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "# Before training the model, configure the learning process via the compile method\n",
    "# Default settings of the optimization algorithms: https://keras.io/optimizers/\n",
    "# sgd = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "# adagrad = optimizers.Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n",
    "adam = optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
    "\n",
    "#sgd = optimizers.SGD(lr = 0.001, momentum = 0.0, decay = 0.0, nesterov = False)\n",
    "modelFit.compile(optimizer = adam,\n",
    "                 loss = 'mean_squared_error',\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "print(modelFit.summary())\n",
    "\n",
    "# Train the model\n",
    "modelEstimate = modelFit.fit(trainX, trainY,\n",
    "                             epochs = 50,\n",
    "                             batch_size = 50,\n",
    "                             verbose = 1,\n",
    "                             validation_data = (validateX, validateY))\n",
    "\n",
    "# make predictions\n",
    "trainPredict = modelFit.predict(trainX)\n",
    "validatePredict = modelFit.predict(validateX)\n",
    "\n",
    "# print the training accuracy and validation loss at each epoch\n",
    "# print the number of models of the network\n",
    "print(modelEstimate.history)\n",
    "print(len(modelFit.layers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Accuracy evaluation of results\n",
    "########################################################\n",
    "\n",
    "# Invert the scaling\n",
    "df_train = np.column_stack((trainPredict, train[:, 1:]))\n",
    "trainPredict2 = scaler.inverse_transform(df_train)\n",
    "\n",
    "df_validate = np.column_stack((validatePredict, validate[:, 1:]))\n",
    "validatePredict2 = scaler.inverse_transform(df_validate)\n",
    "\n",
    "# Plot the errors of the epochs and MSE\n",
    "plt.plot(modelEstimate.history['loss'])\n",
    "plt.plot(modelEstimate.history['val_loss'])\n",
    "#  plt.plot(modelEstimate.history['val_acc'])\n",
    "plt.title('Model Error History')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Training Error', 'Validation Error'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Output final results\n",
    "########################################################\n",
    "\n",
    "# Combine the final datasest - merge the training and validation datasets and rename columns\n",
    "combined_dataframe = pd.concat([pd.DataFrame(trainPredict2), pd.DataFrame(validatePredict2)])\n",
    "combined_dataframe.index = range(len(combined_dataframe))\n",
    "\n",
    "# Add columns names to the data frame with the forecasts\n",
    "names_list = list(inputDataFrame)[1:]\n",
    "names_list[0] = 'lstm_forecast_price'\n",
    "\n",
    "combined_dataframe.columns = names_list\n",
    "\n",
    "actual_value_target = pd.DataFrame(dataframe[:, 0])\n",
    "actual_value_target.columns = ['actual_price']\n",
    "\n",
    "# Output the forecasts. Create the dataframe and write it to a CSV file\n",
    "final_forecast_file = pd.concat([actual_value_target, combined_dataframe], axis = 1)\n",
    "final_forecast_file.to_csv(dataLocation + inputFile + \"_withForecasts\" + \".csv\", sep = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 36",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
