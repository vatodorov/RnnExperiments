{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-79aea48ad776>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;31m# Normalize the data between 0 - 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;31m# Split the data in train and validaiton\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         X = check_array(X, copy=self.copy, warn_on_dtype=True,\n\u001b[1;32m--> 334\u001b[1;33m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    420\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    421\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     41\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     42\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 43\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a trained Recurrent Neural Network (LSTM) to predict Brent price\n",
    "\n",
    "Guide to Keras:\n",
    "    https://keras.io/getting-started/sequential-model-guide/#training\n",
    "\n",
    "@author: valentin\n",
    "\"\"\"\n",
    "\n",
    "## Import the functions and classes we'll need\n",
    "import winsound\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "########### Assign values to parameters\n",
    "\n",
    "# Number of epochs to run\n",
    "_epochs = 2000\n",
    "\n",
    "\n",
    "# Read in the data\n",
    "dataLocation = \"C:/Projects/EAA/Development/Model/\"\n",
    "\n",
    "brentPriceDf = pd.read_csv(dataLocation + \"brent_forecast_variables.csv\")\n",
    "brentPriceDf.head(10)\n",
    "\n",
    "# Remove the forecasts from the data\n",
    "# inputData.BrentPrice[[inputData.Date > 2017-01-01 00:00:00]] = 0\n",
    "\n",
    "# Format \"Date\" field as date\n",
    "brentPriceDf[['date']] = pd.to_datetime(brentPriceDf.date)\n",
    "\n",
    "# Convert the data frame to a Numpy array\n",
    "brentPriceArr = brentPriceDf.iloc[:, 0:].values\n",
    "\n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "# Select random seed\n",
    "randSeed = 789\n",
    "\n",
    "# Provide names of input features\n",
    "inputDataFrame = brentPriceDf\n",
    "inputData = brentPriceArr\n",
    "\n",
    "# Create a Numpy array from the input data\n",
    "dataframe = inputData[:, 1:, ]\n",
    "# xVarColumns = [1, 2, 3]                                   # Select features: BY DEFAULT, it uses all features in columns 1:END for predictors\n",
    "yVarColumns = [0]                                           # select target: The target should always be in the first column\n",
    "number_of_features = len(list(brentPriceDf)) - 2            # Calculate the number of features to be used in the network\n",
    "\n",
    "\n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(randSeed)\n",
    "\n",
    "\n",
    "## NO NEED FOR ARRAYS - Extract the NumPy array from the dataframe and convert the integer values to\n",
    "# floating point values, which are more suitable for modeling with a neural network\n",
    "# dataset = dataframe.values\n",
    "# dataset = dataset.astype('float32')\n",
    "\n",
    "# Normalize the data between 0 - 1\n",
    "scaler = MinMaxScaler(feature_range = (0, 1), copy = True)\n",
    "dataset = scaler.fit_transform(dataframe)\n",
    "\n",
    "# Split the data in train and validaiton\n",
    "trainSize = int(len(dataset) * 2 / 3)\n",
    "train = dataset[0:trainSize, :]\n",
    "validate = dataset[trainSize:len(dataset), :]\n",
    "\n",
    "print(len(train), len(validate), len(dataset))\n",
    "\n",
    "## Modify the data for the LSTM network - The LSTM network expects the input data (X)\n",
    "# to be provided with a specific array structure in the form of: [samples, time steps, features].\n",
    "trainX = train[:, 1:]\n",
    "validateX = validate[:, 1:]\n",
    "\n",
    "trainY = train[:, yVarColumns]\n",
    "validateY = validate[:, yVarColumns]\n",
    "\n",
    "dataframe_length = len(trainY)\n",
    "# dataframe_dim = Need to figure out how to count the columns of the array\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = trainX.reshape(trainX.shape[0], 1, trainX.shape[1])\n",
    "validateX = validateX.reshape(validateX.shape[0], 1, validateX.shape[1])\n",
    "\n",
    "\n",
    "## The LSTM network expects the input data (X) to be provided with a specific\n",
    "# array structure in the form of: [samples, time steps, features]\n",
    "# Define the network\n",
    "modelFit = Sequential()\n",
    "modelFit.add(LSTM(20,\n",
    "                  activation = 'sigmoid',                            # sigmoid, relu, linear, softmax\n",
    "                  input_shape = (1, number_of_features)))\n",
    "modelFit.add(Dropout(.1))\n",
    "modelFit.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "# Before training the model, configure the learning process via the compile method\n",
    "modelFit.compile(optimizer = 'adagrad',                              # adam, adagrad\n",
    "                 loss = 'mean_squared_error',                        # poisson, mean_squared_error, binary_crossentropy\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "print(modelFit.summary())\n",
    "\n",
    "# Train the model\n",
    "modelEstimate = modelFit.fit(trainX, trainY,\n",
    "                             epochs = _epochs,\n",
    "                             batch_size = 1,\n",
    "                             verbose = 1,\n",
    "                             validation_data = (validateX, validateY))\n",
    "\n",
    "# make predictions\n",
    "trainPredict = modelFit.predict(trainX)\n",
    "validatePredict = modelFit.predict(validateX)\n",
    "\n",
    "# print the training accuracy and validation loss at each epoch\n",
    "# print the number of models of the network\n",
    "print(modelEstimate.history)\n",
    "print(len(modelFit.layers))\n",
    "\n",
    "\n",
    "# Invert predictions\n",
    "df_train = np.column_stack((trainPredict, train[:, 1:]))\n",
    "trainPredict2 = scaler.inverse_transform(df_train)\n",
    "\n",
    "df_validate = np.column_stack((validatePredict, validate[:, 1:]))\n",
    "validatePredict2 = scaler.inverse_transform(df_validate)\n",
    "\n",
    "\n",
    "# Plot the errors of the epochs and MSE\n",
    "plt.plot(modelEstimate.history['loss'])\n",
    "plt.plot(modelEstimate.history['val_loss'])\n",
    "#  plt.plot(modelEstimate.history['val_acc'])\n",
    "plt.title('Model Error History')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Training Error', 'Validation Error'])\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "###################################################\n",
    "# Combine the final datasest - merge the training and validation datasets and rename columns\n",
    "combined_dataframe = pd.concat([pd.DataFrame(trainPredict2), pd.DataFrame(validatePredict2)])\n",
    "combined_dataframe.index = range(len(combined_dataframe))\n",
    "\n",
    "# Add columns names to the data frame with the forecasts\n",
    "names_list = list(inputDataFrame)[1:]\n",
    "names_list[0] = 'forecast_brent_price'\n",
    "\n",
    "combined_dataframe.columns = names_list\n",
    "\n",
    "actual_value_target = pd.DataFrame(dataframe[:, 0])\n",
    "actual_value_target.columns = ['actual_brent_price']\n",
    "\n",
    "\n",
    "# Create the dataframe and write it to a CSV file\n",
    "final_forecast_file = pd.concat([actual_value_target, combined_dataframe], axis = 1)\n",
    "final_forecast_file.to_csv(dataLocation + \"Brent_forecast.csv\", sep = ',')\n",
    "\n",
    "\n",
    "# Beep when done with code\n",
    "winsound.Beep(750, 200)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 36",
   "language": "python",
   "name": "myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
